{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1357e2f1-c116-46ae-bd4f-2f77cc3f7a4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://api.coingecko.com/api/v3/coins/bitcoin/market_chart?vs_currency=usd&days=30\"\n",
    "\n",
    "headers = {\"accept\": \"application/json\"}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "\n",
    "#print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf99e6b8-2d46-4588-b43c-db7fefd88c5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "prices_data = response.json()[\"prices\"]\n",
    "market_caps_data = response.json()[\"market_caps\"]\n",
    "total_volumes_data = response.json()[\"total_volumes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b234ab7-4d76-4336-8b87-70ead46b7461",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession \n",
    "    .builder \n",
    "    .appName(\"Spark Project\")\n",
    "    .config(\"spark.driver.memory\", \"8g\") \n",
    "    .config(\"spark.executor.memory\", \"4g\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"2\")\n",
    "    .master(\"local[*]\") \n",
    "    .getOrCreate()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86570262-0b4f-4bc3-9c97-b3a431833de5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "prices_schema = \"timestamp long , price double\"\n",
    "market_caps_schema = \"timestamp long , market_cap double\"\n",
    "total_volumes_schema = \"timestamp long , total_volume double\"\n",
    "\n",
    "\n",
    "prices_df = spark.createDataFrame(data = prices_data , schema = prices_schema)\n",
    "market_caps_df = spark.createDataFrame(data = market_caps_data , schema = market_caps_schema)\n",
    "total_volumes_df = spark.createDataFrame(data = total_volumes_data , schema = total_volumes_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "257e0683-560d-4d81-9df1-d28cc47e51d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "spark.conf.set(\"spark.sql.adaptive.Enabled\" , False)\n",
    "spark.conf.set(\"spark.sql.coalescePartitions.enabled\" , False)\n",
    "spark.conf.set(\"spark.sql.autoBroacastJoinThreshold\" , False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55662bbe-4605-4d8e-8304-5e6d3dbbc613",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "bitcoin_df1 = broadcast(prices_df).join(broadcast(market_caps_df) ,[\"timestamp\"] , \"inner\").join(broadcast(total_volumes_df) ,[\"timestamp\"] , \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0be89bad-1856-48e1-b3a7-47e4f84574f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [timestamp#2L, price#3, market_cap#7, total_volume#11]\n",
      "   +- BroadcastHashJoin [timestamp#2L], [timestamp#10L], Inner, BuildRight, false, true\n",
      "      :- Project [timestamp#2L, price#3, market_cap#7]\n",
      "      :  +- BroadcastHashJoin [timestamp#2L], [timestamp#6L], Inner, BuildRight, false, true\n",
      "      :     :- Filter isnotnull(timestamp#2L)\n",
      "      :     :  +- Scan ExistingRDD[timestamp#2L,price#3]\n",
      "      :     +- Exchange SinglePartition, EXECUTOR_BROADCAST, [plan_id=45]\n",
      "      :        +- Filter isnotnull(timestamp#6L)\n",
      "      :           +- Scan ExistingRDD[timestamp#6L,market_cap#7]\n",
      "      +- Exchange SinglePartition, EXECUTOR_BROADCAST, [plan_id=49]\n",
      "         +- Filter isnotnull(timestamp#10L)\n",
      "            +- Scan ExistingRDD[timestamp#10L,total_volume#11]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#bitcoin_df1.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8728d646-90d6-40a1-a6f7-184322142959",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_unixtime , col , round , to_date\n",
    "\n",
    "bitcoin_df2 = ( bitcoin_df1.withColumn(\"Date\" , to_date(from_unixtime(col(\"timestamp\") / 1000)))\n",
    "                        .withColumn(\"price\",round(col(\"price\") , 2))\n",
    "                        .withColumn(\"market_cap\",round(col(\"market_cap\") / 1e12 , 2))\n",
    "                        .withColumn(\"total_volume\",round(col(\"total_volume\") / 1e9 , 2))\n",
    "                        .drop(\"timestamp\")\n",
    "                        .select(\"Date\" , \"price\" , \"market_cap\" , \"total_volume\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fa405ef-38fe-47a5-9edc-69b2372a9c6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg,sum,asc\n",
    "\n",
    "bitcoin_df2 = (bitcoin_df2.groupBy(col(\"Date\"))\n",
    "              .agg(round(avg(\"price\"),2).alias(\"Average_Daily_Price\"),\n",
    "                   round(avg(\"market_cap\"),2).alias(\"Average_Market_Cap\"),\n",
    "                   round(sum(\"total_volume\"),2).alias(\"Daily_Volume\")\n",
    "                )\n",
    "              .orderBy(col(\"Date\").asc())\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "227d8b4f-e26b-4932-bbc6-b4beab74d942",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lag , when , col, round\n",
    "\n",
    "Window_Specs = Window.orderBy(col(\"Date\").asc())\n",
    "\n",
    "def percentage_change(df , colName , alias):\n",
    "    lag_name = \"Lag_\" + colName\n",
    "    lag_func = lag(colName).over(Window_Specs)\n",
    "\n",
    "    return (\n",
    "            df.withColumn(lag_name , lag_func)\n",
    "              .withColumn(\"Lag_Date\" , lag(\"Date\").over(Window_Specs))\n",
    "              .withColumn(alias , when(col(lag_name).isNull() , 0).when(col(lag_name) == 0 , 0).otherwise(round(((col(colName) - col(lag_name)) / col(lag_name)) * 100 ,2)))\n",
    "              .select(\"Date\" , \"Lag_Date\" , alias)\n",
    "    ) \n",
    "\n",
    "bitcoin_price = percentage_change(bitcoin_df2 , \"Average_Daily_Price\" , \"Daily_Price_Change_Pct\")\n",
    "bitcoin_market_cap = percentage_change(bitcoin_df2 , \"Average_Market_Cap\" , \"Daily_Market_Cap_Change_Pct\")\n",
    "bitcoin_volume = percentage_change(bitcoin_df2 , \"Daily_Volume\" , \"Daily_Volume_Change_Pct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48eddddf-c65c-463a-9838-615f2c8c82ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import asc , desc , col \n",
    "\n",
    "def sharpest_values(changefunc , pctname,colName , mode):\n",
    "\n",
    "    if mode == \"increase\":\n",
    "        col_ord = col(pctname).desc()\n",
    "    else:\n",
    "        col_ord = col(pctname).asc()\n",
    "\n",
    "    sharpest_row = changefunc.orderBy(col_ord).limit(1).select(\"Date\",\"Lag_Date\").alias(\"s\")\n",
    "    \n",
    "    return (\n",
    "            bitcoin_df2.alias(\"b\").join(sharpest_row,\n",
    "                (col(\"b.Date\") == col(\"s.Date\")) | (col(\"b.Date\") == col(\"s.Lag_Date\")), \"inner\").select(col(\"b.Date\") , col(colName))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cf22db4-30fb-4047-880c-ddca156dc99e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bitcoin_price_increase_values = sharpest_values(bitcoin_price ,\"Daily_Price_Change_Pct\",\"Average_Daily_Price\" , \"increase\" )\n",
    "bitcoin_market_cap_increase_values = sharpest_values(bitcoin_market_cap ,\"Daily_Market_Cap_Change_Pct\",  \"Average_Market_Cap\" , \"increase\")\n",
    "bitcoin_volume_increase_values = sharpest_values(bitcoin_volume ,\"Daily_Volume_Change_Pct\",\"Daily_Volume\" , \"increase\")\n",
    "\n",
    "bitcoin_price_decrease_values = sharpest_values(bitcoin_price ,  \"Daily_Price_Change_Pct\",\"Average_Daily_Price\" , \"decrease\")\n",
    "bitcoin_market_cap_decrease_values = sharpest_values(bitcoin_market_cap , \"Daily_Market_Cap_Change_Pct\",\"Average_Market_Cap\" , \"decrease\")\n",
    "bitcoin_volume_decrease_values = sharpest_values(bitcoin_volume , \"Daily_Volume_Change_Pct\",\"Daily_Volume\" , \"decrease\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_lists(df , col1 , col2):\n",
    "    l1 = df.select(col1 , col2).toLocalIterator()\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in l1:\n",
    "        x.append(i[col1].strftime('%Y - %m - %d'))\n",
    "        y.append(i[col2])\n",
    "        \n",
    "    return x , y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_increase_x , price_increase_y = df_to_lists(bitcoin_price_increase_values, \"Date\" , \"Average_Daily_Price\")\n",
    "#price_decrease_x , price_decrease_y = df_to_lists(bitcoin_price_decrease_values,  \"Date\" , \"Average_Daily_Price\")\n",
    "\n",
    "#market_cap_increase_x , market_cap_increase_y = df_to_lists(bitcoin_market_cap_increase_values, \"Date\", \"Average_Market_Cap\")\n",
    "#market_cap_decrease_x , market_cap_decrease_y = df_to_lists(bitcoin_market_cap_decrease_values, \"Date\", \"Average_Market_Cap\")\n",
    "\n",
    "#volume_increase_x,volume_increase_y = df_to_lists(bitcoin_volume_increase_values, \"Date\" , \"Daily_Volume\")\n",
    "#volume_decrease_x,volume_decrease_y = df_to_lists(bitcoin_volume_decrease_values, \"Date\" , \"Daily_Volume\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "457ba00f-9217-46a9-a3b5-e8d7ce795e6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "def graphs(mode , xcol , ycol ,name):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(xcol, ycol , marker = \"o\")\n",
    "    plt.title(f\"Sharpest {name} {mode}\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(f\"{name}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "\n",
    "with PdfPages(\"/opt/airflow/outputs/Bitcoin_Analysis_Report.pdf\") as pdf:\n",
    "    graphs(\"Increase\" , price_increase_x , price_increase_y , \"Price\")\n",
    "    pdf.savefig()\n",
    "    plt.close()\n",
    "    \n",
    "    #graphs(\"Decrease\" , price_decrease_x , price_increase_y , \"Price\")\n",
    "    #pdf.savefig()\n",
    "    #plt.close()\n",
    "\n",
    "    #graphs(\"Increase\" , market_cap_increase_x , market_cap_increase_y , \"Market Cap\")\n",
    "    #pdf.savefig()\n",
    "    #plt.close()\n",
    "    \n",
    "    #graphs(\"Decrease\" , market_cap_decrease_x , market_cap_decrease_y , \"Market Cap\")\n",
    "    #pdf.savefig()\n",
    "    #plt.close()\n",
    "\n",
    "    #graphs(\"Increase\" , volume_increase_x , volume_increase_y , \"Volume\")\n",
    "    #pdf.savefig()\n",
    "    #plt.close()\n",
    "    \n",
    "    #graphs(\"Decrease\" , volume_decrease_x , volume_decrease_y , \"Volume\")\n",
    "    #pdf.savefig()\n",
    "    #plt.close()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 906823833874214,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Apache_Spark_Mini_Project",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
